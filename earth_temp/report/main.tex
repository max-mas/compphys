\documentclass[a4paper,DIV=12,english]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{bookmark}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xurl}
\usepackage[sorting=none, style=numeric-comp]{biblatex}
\addbibresource{ref.bib}
\usepackage{csquotes}
\usepackage[dvipsnames]{xcolor}
\usepackage[num]{isodate}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{tikz}
%\usepackage{pgfplots}
    %\usepgfplotslibrary{fillbetween}
\usepackage{svg}
\usepackage{braket}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{placeins}
%\setlength\parindent{0pt}
\usepackage{wrapfig}
\usepackage{float}


% Fakesection
\newcommand{\fakesection}[1]{%
    \par\refstepcounter{section}                                        % Increase section counter
    \sectionmark{#1}                                                    % Add section mark (header)
    \addcontentsline{toc}{section}{\protect\numberline{\thesection}#1}  % Add section to ToC
    % Add more content here, if needed.
} 

\renewcommand{\footrulewidth}{0.5pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{}

\fancyfoot[C]{Computational Physics: Eigenvalue Problems 1}
\fancyfoot[R]{\thepage}

\title{Computational Physics: Eigenvalue Problems 1}
\author{Stockholm University, Spring Term 2024 \\Max Maschke}
\date{Apr 11 2024}


\begin{document}
\maketitle


\tableofcontents
\newpage


\newpage
\section{Introduction}
\cite{github}

\section{Discretising the 1D Schrödinger Equation}
The time independent Schrödinger equation (SEQ) is famously an eigenvalue problem: 
\begin{equation}\label{SEQ}
    \mathcal{H}\ket{\psi} = E\ket{\psi}
\end{equation}
For a single particle and moving to real-space representation, using the wave function $\psi(x) = \braket{x|\psi}$, it assumes the form 
\begin{equation}
    \left(-\frac{\hbar^2}{2m}\partial_x^2 + V(x)\right)\psi(x) = E\psi(x)
\end{equation}
where $V(x)$ is the real-space representation of the potential operator $\mathcal{V}$, which is a diagonal operator in this basis. $\mathcal{H}$ itself is, however, not diagonal, which is why, considering the difficulty of obtaining analytical solutions to the SEQ, one might be interested in numerical approaches to the eigenproblem posed.

The first necessary step is to translate the infinite dimensional problem stated above into a finite basis. Naturally, this involves some sort of approximation. Intuitively, one possible approach is to go from the infinite dimensional eigenbasis of the position operator $\mathcal{X}$
\begin{equation}\
    \mathcal{X}\ket{x} = x\ket{x},\quad \braket{x'|x}=\delta(x'-x), \quad x \in \mathbb{R}
\end{equation}
to a discretised set of positions $x_i$ with an associated discrete position operator $X$
\begin{equation}
    X \ket{x_i} = x_i \ket{x_i}, \quad \braket{x_j|x_i} = \delta_{ij}, \quad i \in \mathbb{N}.
\end{equation}
If we additionally impose a maximum and minimum value of $x_i$ (N.B.\ this implies assuming $\psi(x)=0$ outside for the purposes of finding solutions to the SEQ) and require neighbouring positions to have constant spacing $h$, the set of allowed positions assumes a finite number $n$:
\begin{equation}
    x_i = x_\text{min} + i \cdot h, \quad i = 0, \dots, n -1
\end{equation}
It will be convenient to set $x_\text{min} = -x_\text{max}$ henceforth. For a given $n$, that yields
\begin{equation}
     h = 2x_\text{max}/(n-1).
\end{equation}
What we have achieved by moving to this finite basis is that, if we can map $\mathcal{H}$ to its finite representation $H\in\mathbb{C}^n$,~\eqref{SEQ} can be represented as a matrix equation, which allows us to unleash the whole toolbox of numerical linear algebra onto it. Moreover, any real function $\mathcal{F}$ on $[-x_\text{max}, x_\text{max}]$ can be represented by a vector $f\in\mathbb{R}^n$ by the identification
\begin{equation}
    f = \sum_i f(x_i) e_i.
\end{equation}
Naturally, this also applies to the wave function, meaning a physical state $\ket{\psi}$ can be approximately represented in our finite dimensional space. The correct choice for the inner product is the standard inner product, the correct norm is the euclidean norm.

As previously mentioned, the potential operator $\mathcal{V}$ and respectively its finite representation $V$ are diagonal in the real-space basis, i.e.
\begin{equation}
    \braket{x_j|V|x_i} = \delta_{ij}V(x_i) \Rightarrow V = \begin{bmatrix}
        V(x_0) & & & \\
        & V(x_1) & & \\
        & & \ddots & \\
        & & & V(x_{n-1})
    \end{bmatrix}.
\end{equation}
The same is not true for the momentum operator $\mathcal{P}$ or, respectively, $P$. It is a differential operator in position space and thus has no \enquote{exact} representation in the finite basis. However, its action on a continuous state can be approximated in the discrete case using a finite-difference formula for the second derivative. Two examples for centred finite difference formulae are the three- and five-point formulae:
\begin{align}
    f''(x_i) &= \frac{1}{h^2}\left(f(x_{i-1}) - 2f(x_i) + f(x_{i+1})\right) + O(h^2) \\
    f''(x_i) &= \frac{1}{h^2}\left(-\frac{1}{12}f(x_{i-2}) + \frac{4}{3}f(x_{i-1}) - \frac{5}{2}f(x_i) + \frac{4}{3}f(x_{i+1}) -\frac{1}{12}f(x_{i+2}) \right) + O(h^4)
\end{align}
Note that the coefficients of the $f(x_j)$ in both of these formulae are symmetric around the centre point. This is important because we want to translate the Hermitian operator $(\mathcal{P}^2)^\dag = \mathcal{P}^2$ into an Hermitian (actually symmetric because real-valued) matrix $(P^2)^H = P^2$, which, using the five-point formula, has the form
\begin{equation}
    P^2 = -\frac{\hbar^2}{h^2}\begin{bmatrix}
        - \frac{5}{2} & \frac{4}{3} & -\frac{1}{12}& & & \\
        \frac{4}{3} & - \frac{5}{2} & \frac{4}{3} &-\frac{1}{12} & & \\
        -\frac{1}{12} & \frac{4}{3} & - \frac{5}{2} & \frac{4}{3} &-\frac{1}{12} & \\
        & -\frac{1}{12} & \frac{4}{3} & - \frac{5}{2} & \frac{4}{3} &-\frac{1}{12}  \\
        & & \ddots & \ddots & \ddots & \ddots & \ddots \\
        & & & -\frac{1}{12} & \frac{4}{3} & - \frac{5}{2} & \frac{4}{3} &-\frac{1}{12}
    \end{bmatrix}.
\end{equation}

This way, for any given potential, we obtain a real, symmetric matrix
\begin{equation}
    H = \frac{1}{2m}P^2 + V.
\end{equation}
If we can find its eigenpairs numerically, we will obtain an approximate solution to the full SEQ. Note that while the full equation will usually have infinitely many solutions, we can only expect to find as many as we allow discrete positions, i.e. $n$. Furthermore, due to the sampling theorem, we should not expect to be able to represent quickly oscillating wave functions.

\section{Numerical Eigenproblem Solvers}
Analytically, eigenvalues may be obtained by finding the roots of the characteristic polynomial of a matrix $A$:
\begin{equation}
    \det(A - \lambda I) = 0
\end{equation}
One then gets the corresponding eigenvectors by solving
\begin{equation}
    (A - \lambda I) x = 0.
\end{equation}
The problem with this approach for numerical use is that the determinant is highly non-linear in the matrix elements $a_{ij}$. Small errors can thus propagate and yield hugely wrong eigenvalues. Other methods are thus needed.

One example for an efficient and stable eigensolver is the $QR$-algorithm, based on the $QR$ decomposition of a matrix $A$:
\begin{equation}
    A = QR,
\end{equation}
where $Q\in \text{U}(n)$ and $R$ upper triangular. The idea is to iteratively apply orthogonal or, respectively, unitary transformations to $A$ that turn the elements below the diagonal zero. Such transformations are, for example, Householder transformations or Givens rotations. When this has been achieved, the eigenvalues are the diagonal elements.

Eigenvalue solvers usually have complexity $O(n^3)$. If not all eigenvalues are needed, another possible approach that might save some resources is inverse iteration. 

\newpage
\FloatBarrier
\fakesection{References}
\printbibliography


\end{document}
